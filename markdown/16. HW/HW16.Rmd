---
title: "HW (Week 16) - BACS"
author: 'Author: 110077432'
date: '`r Sys.Date()`'
output:
  pdf_document: default
header-includes:
- \usepackage{fancyhdr}
- \pagestyle{fancy}
- \fancyhead[CO,CE]{110077432}
- \fancyfoot[CO,CE]{  }
- \fancyfoot[LE,RO]{\thepage}
- \lhead{6/2/2022}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Let's firstly run the code given in the instructions to this homework:
```{r}
# Load the data and remove missing values
cars <- read.table("auto-data.txt", header=FALSE, na.strings = "?")
names(cars) <- c("mpg", "cylinders", "displacement", "horsepower", "weight",
                 "acceleration", "model_year", "origin", "car_name")
cars$car_name <- NULL
cars <- na.omit(cars)

# Shuffle the rows of cars
set.seed(27935752)
cars <- cars[sample(1:nrow(cars)),]

# Create a log transformed dataset also
cars_log <- with(cars, data.frame(log(mpg), log(cylinders), log(displacement),
log(horsepower), log(weight), log(acceleration), model_year, origin))

# Linear model of mpg over all the variables that don’t have multicollinearity
cars_lm <- lm(mpg ~ weight + acceleration + model_year + factor(origin), data=cars)

# Linear model of log mpg over all the log variables that don’t have multicollinearity
cars_log_lm <- lm(log.mpg. ~ log.weight. + log.acceleration. + model_year +
                    factor(origin), data=cars_log)

# Linear model of log mpg over all the log variables, including multicollinear terms!
cars_log_full_lm <- lm(log.mpg. ~ log.cylinders. + log.displacement. +
                         log.horsepower. + log.weight. + log.acceleration. +
                         model_year + factor(origin), data=cars_log)
```

# Question 1

Let's split the cars_log data into a training and testing set, with a 70:30 split:
```{r}
set.seed(20220602)
train_indices <- sample(1:nrow(cars_log), size=0.70*nrow(cars_log))
```

## a) Retraining cars_log_lm on the training dataset

As we may remember from the finding of week 11/12, a few variables were raising a multicollinearity issue. Therefore, I will include in the models only those variables that were left at the end of the VIF analysis(log.weight., log.acceleration., model_year, origin):
```{r}
train_set <- cars_log[train_indices,]
lm_trained <- lm(log.mpg. ~ log.weight. + log.acceleration. + model_year +
                   factor(origin), data = train_set)

estimates <- lm_trained$coefficients
```

Please find below the _estimated coefficients_ of the trained model:
```{r, warning = FALSE, message = FALSE}
knitr::kable(round(estimates, 2), caption = "Coefficients of the trained model",
             col.names = NULL)
```

## b) Predicting the log.mpg. of the test data using lm_trained

```{r}
test_set <- cars_log[-train_indices,]
mpg_prediction <- predict(lm_trained, test_set)
```

Let's see what is the _in-sample MSE_ of the trained model:
```{r}
round(mean(residuals(lm_trained)^2), 5)
```

While the _out-of-sample MSE_ of the test dataset is:
```{r}
mpg_actual <- test_set$log.mpg.
round(mean((mpg_prediction - mpg_actual)^2), 5)
```

## c) Showing a dataframe of the test set's actual log.mpg., predicted values, and their difference
```{r}
prediction_error <- test_set$log.mpg. - mpg_prediction
comprehensive <- data.frame(cbind(test_set$log.mpg.,
                                  mpg_prediction, prediction_error))

names(comprehensive) <- c("Actual", "Predicted", "Error")

knitr::kable(head(comprehensive, 10),
             caption = "Top 10 rows of actual vs predicted values and error")
```

# Question 2

## a) Reporting the MSE of the 3 given regression models
```{r, warning = FALSE, message = FALSE}
cars_lm_mse <- round(mean(residuals(cars_lm)^2), 5)
cars_log_lm_mse <- round(mean(residuals(cars_log_lm)^2), 5)
cars_log_full_lm_mse <- round(mean(residuals(cars_log_full_lm)^2), 5)

mse <- rbind(cars_lm_mse, cars_log_lm_mse, cars_log_full_lm_mse)

knitr::kable(mse, caption = "MSEs of the three given linear models ")
```

The results show that the model with log.mpg. regressed over all the other variables, including the multicollinear terms, has the _lowest (best) MSE_. The linear model of mpg over all the variables that don't have multicollinearity holds the _highest (worst) MSE_.

\newpage

## b) Writing a function that performs k-fold cross-validation
```{r}
# Function for calculating the mse_oos across all folds
k_fold_mse <- function(dataset, k=10, mod, y) {
  fold_pred_errors <- sapply(1:k, function(i) {
    fold_i_pe(i, k, dataset, mod, y)
  })
  pred_errors <- unlist(fold_pred_errors)
  mean(pred_errors^2)
}

# Function for calculating the prediction error for fold i out of k
fold_i_pe <- function(i, k, dataset, mod, y) {
  folds <- cut(1:nrow(dataset), k, labels = FALSE)
  test_indices <- which(folds == i)
  test_set <- dataset[test_indices, ]
  train_set <- dataset[-test_indices, ]
  trained_model <- lm(mod$terms, data = train_set)
  actuals <- y[test_indices]
  predictions <- predict(trained_model, test_set)
  actuals - predictions
}
```

### i) Reporting the out-of-sample MSE for cars_lm using k_fold_mse()
```{r}
k_fold_mse(dataset = cars, mod = cars_lm, y = cars$mpg)
``` 

### iii) Reporting the out-of-sample MSE for cars_log_lm using k_fold_mse()
```{r}
round(k_fold_mse(dataset = cars_log, mod = cars_log_lm,
                 y = cars_log$log.mpg.), 5)
```

As we can see, the MSE has become much smaller, meaning that the _cars_log_lm model can predict better than cars_lm_. Certainly, the non-linearity of certain variables was _harming_ the predictions.

### iii) Reporting the out-of-sample MSE for cars_log_lm_full using k_fold_mse()
```{r}
round(k_fold_mse(dataset = cars_log, mod = cars_log_full_lm,
                 y = cars_log$log.mpg.), 5)
```

It seems like the _multicollinearity issue does not affect the predictions_ majorly. The MSE is even a little smaller actually.

## c) Verifying if k_fold_mse() can do as many as 392 folds on cars_log_lm
```{r}
round(k_fold_mse(dataset = cars_log, k = 392, mod = cars_log_lm,
                 y = cars_log$log.mpg.), 5)
```

As we can see, the k_fold_mse() function _can report the MSE_ even if we consider as many fold as the number of rows in our dataset.