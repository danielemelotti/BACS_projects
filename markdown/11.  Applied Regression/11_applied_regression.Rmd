---
title: "HW (Week 11) - BACS - Applied Regression"
author: 'Author: 110077432'
date: "4/29/2022"
output:
  pdf_document: default
header-includes:
- \usepackage{fancyhdr}
- \pagestyle{fancy}
- \fancyhead[CO,CE]{110077432}
- \fancyfoot[CO,CE]{  }
- \fancyfoot[LE,RO]{\thepage}
- \lhead{4/29/2022}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require(ggplot2)
require(broom)
require(gridExtra)
require(car)
require(dplyr)
```

# Question 1

First of all, let's load the dataset:
```{r, tidy=TRUE, tidy.opts = list(blank = FALSE, width.cutoff = 60), warning = FALSE}
auto <- read.table("../../data/auto-data.txt", header=FALSE, na.strings = "?")

names(auto) <- c("mpg", "cylinders", "displacement", "horsepower", "weight",
"acceleration", "model_year", "origin", "car_name") 
```

Now, we can create a new dataset that log-transforms most of the variables from our dataset:
```{r}
cars_log <- with(auto, data.frame(log(mpg), log(cylinders), log(displacement),
log(horsepower), log(weight), log(acceleration), model_year, origin))
```

## a) Running a regression on cars_log, with log.mpg. dependent on all other variables
```{r, tidy=TRUE, tidy.opts = list(blank = FALSE, width.cutoff = 60)}
regr <- lm(log.mpg. ~ log.cylinders. + log.displacement. + log.horsepower. + log.weight. + log.acceleration. + model_year + origin, data = cars_log)
summary(regr)
```

### i) Which log-transformed factors are significant at 10% significance?

The regression summary shows that horsepower, weight, acceleration, model year and origin are statistically significant within 10% significance.

### ii) Do some new factors have a significant effect on mpg? Why might this be?

In comparison with the previous assignment, we have turned horsepower and acceleration to be significant. This might be because the log-transformation has turned our data and residuals into a more linear shape, which means that the results obtained from the regression should be more reliable. Because of the issue of non-linearity, some of the coefficients and residuals might have not been symmetrically distributed before, but now we improved this.

### iii) Which factors still have insignificant or opposite (from correlation) effects on mpg? Why might this be?

We see that the variables cylinders and displacement are still insignificant. 
We also see that acceleration has a negative coefficient in the regression output, while in the correlation table from HW 10 the relation between mpg and acceleration was positive. This issue is probably a consequence of multicollinearity. For reference, please find the correlation table below:

```{r, warning = FALSE}
knitr::kable(round(cor(na.omit(auto[-9])), 2), caption = "Correlation table")
```


## b) Taking a closer look at weight

### i) Creating a regression of mpg over weight from the original dataset
```{r}
regr_wt <- lm(mpg ~ weight, data = auto)
summary(regr_wt)
```

### ii) Creating a regression of log.mpg. over log.weight. from cars_log
```{r}
regr_wt_log <- lm(log.mpg. ~ log.weight., data = cars_log)
summary(regr_wt_log)
```

### iii) Visualizing the residuals of both regressions

First, let's plot the density of the residuals. We need to convert the results of the regressions to dataframes using fortify(), so that they can be plotted using ggplot2:
```{r}
mod <- fortify(regr_wt)
mod_log <- fortify(regr_wt_log)

p1 <- ggplot(mod, aes(x = .resid)) +
  geom_density(color = "dodgerblue3") +
  geom_vline(xintercept = mean(mod$.resid), lty = "dashed", size = 0.75) +
  labs(x = "raw residuals") +
  theme_classic()

p2 <- ggplot(mod_log, aes(x = .resid)) +
  geom_density(color = "tomato4") +
  geom_vline(xintercept = mean(mod_log$.resid), lty = "dashed", size = 0.75) +
  labs(x = "log-transformed residuals") +
  theme_classic()

ordered <- arrangeGrob(p1, p2, nrow = 2)

ggsave("plot_1.png", ordered)
```

```{r plot_1&2, echo=FALSE, fig.cap="Density of Raw residuals vs Log-transformed residuals", out.width = '60%', out.height = '60%', fig.height = 4, fig.width = 7, fig.align='center'}
knitr::include_graphics("plot_1.png")
```

Next, we can create the scatterplot of log.weight. vs residuals:
```{r}
p3 <- ggplot(mod, aes(x=weight, y=.resid)) +
    geom_point(size=3, color = "dodgerblue3") +
    geom_hline(yintercept = mean(mod_log$.resid), color = "red", size = 1.5) +
    theme_classic()

p4 <- ggplot(mod_log, aes(x=log.weight., y=.resid)) +
    geom_point(size=3, color = "tomato4") +
    geom_hline(yintercept = mean(mod_log$.resid), color = "red", size = 1.5) +
    theme_classic()

ordered <- arrangeGrob(p3, p4, ncol = 1)
# ggsave("plot_2.png", ordered)
```

```{r plot_3&4, echo=FALSE, fig.cap="Scatterplot of Raw residuals vs Log-transformed residuals", out.width = '60%', out.height = '60%', fig.align='center'}
knitr::include_graphics("plot_2.png")
```

The residuals seem decently centered around zero after we perform log-transformation, with only a few extreme cases. Certainly, we note an improvement from the specification with the original variables, and we see that now the regression requirements related to residuals are much more likely to be met.

### iv) Which regression produces better distributed residuals?

The regression with logs produces better distributed residuals, we can see this from the density plots in iii), where the density for that regression has a more normal and symmetrical distribution than the density for the regression performed on original values. We can also see it from the scatterplots, which were described just before.

### v) How do you interpret the slope of log.weight. vs log.mpg.?

Looking at the regression summary, we can conclude that a 1% increase in weight leads to a 1.0583% lower mileage per gallon. Or more generally, a 1% change in weight leads to a -1.0583% change in mpg. Therefore, the heavier the car, the more fuel it consumes, and viceversa.

## c) Examining the 95% CI of the slope of log.weight. vs log.mpg.

### i) Creating a bootstrapped CI

Here's the bootstrapped interval visualization:
```{r fig, tidy=TRUE, tidy.opts = list(blank = FALSE, width.cutoff = 60), fig.align = "center", fig.height = 4, fig.width = 6}
# Empty plot canvas
plot(log(auto$weight), log(auto$mpg), col=NA, pch=19, main = "Bootstrapped 95% CI")

# Function for single resampled regression line
boot_regr <- function(model, dataset) {
  boot_index <- sample(1:nrow(dataset), replace=TRUE)
  data_boot <- dataset[boot_index,]
  regr_boot <- lm(model, data=data_boot)
  abline(regr_boot, lwd=1, col=rgb(0.7, 0.7, 0.7, 0.5))
  regr_boot$coefficients
}

# Bootstrapping for confidence interval
set.seed(26071996)
coeffs <- replicate(300, boot_regr(log(mpg) ~ log(weight), auto))

# Plot points and regression line
points(log(auto$weight), log(auto$mpg), col="paleturquoise2", pch=19)
abline(a=mean(coeffs["(Intercept)",]), 
       b=mean(coeffs["log(weight)",]), lwd=2)
```

Here's the values of the 95% CI:
```{r}
CI_95 <- quantile(coeffs["log(weight)",], c(0.025, 0.975))
knitr::kable(CI_95, caption = "Bootstrapped 95% CI")
```

And here is the density of the CI of the coefficient:
```{r d_i, tidy=TRUE, tidy.opts = list(blank = FALSE, width.cutoff = 60), fig.align = "center", fig.height = 4, fig.width = 6}
plot(density(coeffs["log(weight)",]), xlim=c(-1.5, -0.5), col = "red", main = "Bootstrapped 95% CI density")
abline(v=quantile(coeffs["log(weight)",],  c(0.025, 0.975)))
```


### ii) Verifying my resuts with a traditionally computed CI

We can compute the CI with the aid of confint():
```{r}
knitr::kable(confint(regr_wt_log), caption = "Traditionally computed 95% CI")
```

As we can see, the confidence interval is fairly similar to the bootstrapped one.

# Question 2

We run the following regression:
```{r}
regr_log <- lm(log.mpg. ~ log.cylinders. + log.displacement. + log.horsepower. +
  log.weight. + log.acceleration. + model_year +
  factor(origin), data=cars_log)
summary(regr_log)
```

## a) Computing VIF of log.weight. using R squared

First, we must run the regression with log.weight. as the outcome variable:
```{r, tidy=TRUE, tidy.opts = list(blank = FALSE, width.cutoff = 60)}
regr_log_weight <- lm(log.weight. ~ log.cylinders. + log.displacement. + log.horsepower. +
  log.acceleration. + model_year +
  factor(origin), data=cars_log)
summary(regr_log_weight)
```

Then, we can extract the R squared from the regression summary and calculate the VIF:
```{r}
r2_weight <- summary(regr_log_weight)$r.squared
vif_weight <- 1 / (1 - r2_weight)

vif_weight
```

As we can see, the value of VIF is very high, well above the 4 or 5 threshold. This means that weight is a highly collinear variable.

## b) Using the Stepwise VIF selection procedure

### i) Computing the VIF of all independent variables
```{r}
vif_all <- vif(regr_log)
knitr::kable(vif_all, caption = "VIF of all independent variables of regr_log")
```

### ii) Removing the independent variable with largest VIF (and greater than 5)

Let's create a function that finds the variable for us:
```{r}
high_VIF_remover <- function(reg) {
  vif_data <- data.frame(vif(reg))
  removed <- vif_data %>% filter(GVIF == max(GVIF) & GVIF > 5)
  return(removed)
}

knitr::kable(high_VIF_remover(regr_log), caption = "Variable to be removed")
```

As we can see, displacement has the highest VIF and shall be removed from the data.

### iii) Repeating steps i) and ii) until no more variable has VIF > 5

Let's rerun the regression without the displacement variable, and then check the updated VIF values:
```{r}
regr_log_2 <- lm(log.mpg. ~ log.cylinders. + log.horsepower. +
  log.weight. + log.acceleration. + model_year +
  factor(origin), data=cars_log)

vif_2 <- vif(regr_log_2)

knitr::kable(vif_2, caption = "VIF values of all variables apart from displacement")
```
We see that there are still some high VIF values. Let's proceed onto removing the next one:
```{r}
knitr::kable(high_VIF_remover(regr_log_2), caption = "Next variable to be removed")
```


As we can see, the next variable we should remove is horsepower. Let's do that, rerun our regression, and see the new VIF variables:
```{r, tidy=TRUE, tidy.opts = list(blank = FALSE, width.cutoff = 60)}
regr_log_3 <- lm(log.mpg. ~ log.cylinders. +
  log.weight. + log.acceleration. + model_year +
  factor(origin), data=cars_log)

vif_3 <- vif(regr_log_3)

knitr::kable(vif_3, caption = "VIF values of all variables apart from 
             displacement, and horsepower")
```

We see that there is still a variable with VIF > 5, but we are close to achieving our goal. Let's proceed:
```{r}
knitr::kable(high_VIF_remover(regr_log_3), caption = "Next variable to be removed")
```

We confirm that the next variable to be removed should be cylinders. Let's do that, rerun the regression, and check the new VIF values:
```{r, tidy=TRUE, tidy.opts = list(blank = FALSE, width.cutoff = 60)}
regr_log_4 <- lm(log.mpg. ~ log.weight. + log.acceleration. + model_year +
  factor(origin), data=cars_log)

vif_4 <- vif(regr_log_4)

knitr::kable(vif_4, caption = "VIF values of all variables apart from displacement, 
             horsepower and cylinders")
```

We have finally achieved the goal and removed all variables with VIF > 5.

### iv) Reporting the final regression model and its summary statistics
```{r}
summary(regr_log_4)
```

## c) Have we lost any variables that were previously significant? How much did we hurt our explanation by dropping those variables?

The only removed variable which was significant in our starting specification was horsepower. By removing this variable, as well as displacement and cylinders, we did not really hurt the explanatory power of the model. We can verify it in the following summary:
```{r, warning = FALSE}
R_2_original_model <- summary(regr_log)$r.squared
R_2_modified_model <- summary(regr_log_4)$r.squared
R_2_comparison <- rbind(R_2_original_model, R_2_modified_model)

knitr::kable(R_2_comparison, caption = "Comparison of the R squared values")
```

We notice that the value of R squared is almost the same.

## d) Deducing a couple interesting things from the VIF formula

### i) If an independent variable has no correlation with other independent variables, what would its VIF score be?

Let's recall the VIF formula:

$$
VIF_j = \frac{1}{1-R^2_j} 
$$
Where R squared represents the squared multiple correlation. Since the two variables would not be correlated at all, the R squared value will be 0. In that case the VIF score is likely to be 1 (we would be dividing 1 by 1 - 0).

### ii) Given a regression with only two independent variables, how correlated would they have to be, to get VIF scores of 5 or higher? To get VIF scores of 10 or higher?

In such case we would have to solve the VIF formula for the value of R:

$$
R_j = \sqrt{1 - \frac{1}{VIF_j}}
$$
Therefore, let's set the parameters and see what value of correlation we need in order to have a VIF > 5 and VIF > 10:
```{r, warning = FALSE}
VIF_5 <- 5
VIF_10 <- 10

R_cor_5 <- sqrt(1 - (1 / VIF_5))
R_cor_10 <- sqrt(1 - (1 / VIF_10))
d <- rbind(R_cor_5, R_cor_10)

knitr::kable(d, caption = "Minimum values of R necessary to have a VIF > 5 and VIF > 10")
```

# Question 3

Let's check if the relationship of weight on mpg is different for cars from different origins. To do that, we start by visualizing it. First, we plot all the weights, using different colors and symbols for the three origins:
```{r s_last, tidy=TRUE, tidy.opts = list(blank = FALSE, width.cutoff = 60), fig.align = "center", fig.height = 4, fig.width = 6}
origin_colors = c("blue", "darkgreen", "red")
with(cars_log, plot(log.weight., log.mpg., pch=origin, col=origin_colors[origin]))
title("Scatterplot of origins")
```

## a) Adding three separate regression lines on the scatterplot, one for each of the origins:
```{r last, tidy=TRUE, tidy.opts = list(blank = FALSE, width.cutoff = 60), fig.align = "center", fig.height = 5, fig.width = 7}
origin_colors = c("blue", "darkgreen", "red")
with(cars_log, plot(log.weight., log.mpg., pch=origin, col=origin_colors[origin]))
title("Scatterplot of origins with regression lines by origin")

# Regression line for the US:
cars_us <- subset(cars_log, origin==1)
wt_regr_us <- lm(log.mpg. ~ log.weight., data=cars_us)
abline( wt_regr_us, col=origin_colors[1], lwd=2)

# Regression for EU:
cars_eu <- subset(cars_log, origin==2)
wt_regr_eu <- lm(log.mpg. ~ log.weight., data=cars_eu)
abline(wt_regr_eu, col=origin_colors[2], lwd=2)

# Regression for Japan:
cars_jp <- subset(cars_log, origin==3)
wt_regr_jp <- lm(log.mpg. ~ log.weight., data=cars_jp)
abline(wt_regr_jp, col=origin_colors[3], lwd=2)
```

## b) Do cars from different origins appear to have different weight vs. mpg relationships?

I think that cars from different origins have relationships of weight vs mpg that are not too different, especially in the case of European and Japanese cars. The trend indicates that heavier cars have a lower mpg, and viceversa. We can see that Japanese and European cars tend to have lighter cars which offer a higher mpg, and their regression lines have similar slope. Instead, in the case of US-made cars, we see that they are heavier and more fuel consuming on average, and their regression slope is more negatively steep.