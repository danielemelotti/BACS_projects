---
title: "HW (Week 5) - BACS"
author: 'Author: 110077432'
date: "3/20/2022"
output:
  pdf_document: default
header-includes:
- \usepackage{fancyhdr}
- \pagestyle{fancy}
- \fancyhead[CO,CE]{110077432}
- \fancyfoot[CO,CE]{  }
- \fancyfoot[LE,RO]{\thepage}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Question 1

For reference, this would be the original (starting) scenario:

```{r s, echo=FALSE, fig.cap="Starting scenario: diff = 0.3, sd = 2.9, n = 50, alpha = 0.05", out.width = '60%', fig.align='center', }
knitr::include_graphics("Q1_s.png")
```

## Scenario a)

### i) Would this scenario create systematic or random error (or both or neither)?
In this scenario it is likely that there will be a systematic error, as our colleague "systematically" chose to collect data from young consumers, missing most of the older ones. 

### ii) Which part of the t-statistic would be affected?
I think that the part which would be most influenced is the diff parameter, as the usage time will often be more than the hypothesized mean (we mostly consider young consumers, which are more likely to use a smartwatch). If we assume that our colleague "substituted" the older consumers with young ones, then the n parameter should not change.

### iii) Will it increase our power to reject the null hypothesis?
Increasing the diff parameter leads to an increase in the power to reject the null hypothesis.

### iv) Which kind of error becomes more likely of this scenario?
I think it would be Type I error. As we increase the diff parameter we notice more evidence that we should reject the null hypothesis. This leads us to believe that the null hypothesis is really false. However, this is a result of a faulty data retrieval procedure (which does not consider older customers, who are likely to use the watch less). Perhaps, if the null hypothesis was really true, we would be making a mistake, which would mean Type I error.

```{r s1, echo=FALSE, fig.cap="Scenario a: diff = 0.8, sd = 2.9, n = 50, alpha = 0.05", out.width = '60%', fig.align='center', }
knitr::include_graphics("Q1_a.png")
```

## Scenario b)

### i) Would this scenario create systematic or random error (or both or neither)?
This scenario is likely to create a random error, as the 20 people who were using the wrong device were likely to be random.

### ii) Which part of the t-statistic would be affected?
Certainly, the n parameter would be affected as we would have to remove 20 observations from our dataset. This is likely to increase standard deviation too.

### iii) Will it increase our power to reject the null hypothesis?
It will slightly decrease our power to reject the null hypothesis.

### iv) Which kind of error becomes more likely of this scenario?
I think that this scenario is closer to a situation in which the null hypothesis is really true rather than really false. Evidence shows that we would be more likely to not reject the null hypothesis. Therefore, the kind of error we could encounter would be a Type II error (beta > alpha).

```{r s2, echo=FALSE, fig.cap="Scenario b: diff = 0.3, sd = 3.2, n = 30, alpha = 0.05", out.width = '60%', fig.align='center'}
knitr::include_graphics("Q1_b.png")
```

## Scenario c)

### i) Would this scenario create systematic or random error (or both or neither)?
Perhaps, this scenario could create random error. When we decrease our confidence, we increase the precision of our interval (which becomes narrower); however, by doing so, we reduce the chance that the actual mean is contained in our new interval.

### ii) Which part of the t-statistic would be affected?
The alpha parameter would be affected for sure, as it would increase to 10%.

### iii) Will it increase our power to reject the null hypothesis?
This scenario would lead to a greater power to reject the null hypothesis.

### iv) Which kind of error becomes more likely of this scenario?
I think that in this scenario there would be still more evidence for the null hypothesis to be really true, however, we could be making a mistake if the null hypothesis was really false. The Type II error is more likely to happen even in this scenario (beta > alpha).

```{r s3, echo=FALSE, fig.cap="Scenario c: diff = 0.3, sd = 2.9, n = 50, alpha = 0.1", out.width = '60%', fig.align='center', }
knitr::include_graphics("Q1_c.png")
```

## Scenario d)

### i) Would this scenario create systematic or random error (or both or neither)?
I think that this scenario could potentially create both random error and systematic error, as it would systematically overestimate the usage of old users and systematically underestimate the usage of younger users.

### ii) Which part of the t-statistic would be affected?
It would probably be the sd parameter, as we are creating a greater spread between young consumers data (underreport) and older consumers' data (overreport). I think that the diff parameter could not change significantly; however, this depends on how many consumers are young ad how many are old, and on how much their usage is underestimated or overestimated. I believe that it is quite difficult to assess the change in diff without having a greater knowledge of the data.

### iii) Will it increase our power to reject the null hypothesis?
This scenario, with an increase in standard deviation, will probably decrease our power to reject the null hypothesis.

### iv) Which kind of error becomes more likely of this scenario?
This scenario shows more evidence for not rejecting the null hypothesis, therefore, the Type II error is more likely to happen (beta > alpha).

```{r s4, echo=FALSE, fig.cap="Scenario d: diff = 0.3, sd = 3.9, n = 50, alpha = 0.05", out.width = '60%', fig.align='center', }
knitr::include_graphics("Q1_d.png")
```

# Question 2
## a) Recreating HW4's traditional hypothesis test using t.test() and power.t.test()

### i) Running a t-test:
```{r}
# Importing the data:
verizon <- read.csv("verizon.csv")
```

```{r}
# Running t-test:
t.test(x = verizon$Time, alternative = "greater", mu = 7.6, conf.level = 0.99)
```

### ii) Veryfing the power of the test
```{r, tidy=TRUE, tidy.opts = list(blank = FALSE, width.cutoff = 60)}
mu_0 <- 7.6
mu_1 <- mean(verizon$Time)
stdev <- sd(verizon$Time)

power.t.test(n = length(verizon$Time), delta = mu_1 - mu_0, sd = stdev, sig.level = 0.01, type = "one.sample", alternative = "one.sided")
```
As we can see, the power to reject the null hypothesis is equal to 59.19%.

## b) Re-examining the problem

### i) Retrieve the original t-value from above
```{r, tidy=TRUE, tidy.opts = list(blank = FALSE, width.cutoff = 60)}
t_stat <- round(unname(t.test(x = verizon$Time, alternative = "greater", mu = 7.6, conf.level = 0.99)[[1]]), 4)
t_stat
```

### ii) Bootstrapping the null and alternative t-distributions
```{r}
bootstrap_null_alt <- function(sample0, hyp_mean) {
  resample <- sample(sample0, length(sample0), replace=TRUE)
  resample_se <- sd(resample) / sqrt(length(resample))
  t_stat_alt <- (mean(resample) - hyp_mean) / resample_se
  t_stat_null <- (mean(resample) - mean(sample0)) / resample_se
  c(t_stat_alt, t_stat_null)
}

set.seed(450)

boot_t_stats <- replicate(10000, bootstrap_null_alt(verizon$Time, mu_0))
t_alt <- boot_t_stats[1,]
t_null <- boot_t_stats[2,]
ci_99 <- quantile(t_null, probs=c(0.005, 0.995)) # 99% cutoff!
```

Plotting the null and alternative distributions:
```{r pl}
plot(density(t_alt), xlim=c(-7,6), main =  "Null & Alternative distributions")
lines(density(t_null), lty="dashed")
abline(v=t_stat, col="red")
abline(v=ci_99, lty="dotted", col = "blue") # 99% cutoff
```

### iii) Finding the 99% cutoff value for critical null values of t (from the bootstrapped null)
The 99% cutoff was computed already in Q2) b) ii), therefore I am only reporting its values here, as there's no need to recalculate it:
```{r}
# Calculating the 99% cutoff of bootstrapped null values:
ci_99

# Our original t-stat:
t_stat
```
As our t-statistic trespasses the 99% cutoff value on the right bound, we could reject the null hypothesis and claim that the mean repair time of Verizon is greater than 7.6 minutes.

### iv) Computing the p-value and power of our bootstrapped test
```{r}
# The p-value is:
set.seed(600)
null_probs <- ecdf(t_null)
one_tailed_pvalue <- 1 - null_probs(t_stat)
one_tailed_pvalue
```
As we can see, the p-value is smaller than the significance level 0.01, therefore this makes our t-statistics significant, allowing to confirm the rejection of the null hypothesis. 

```{r}
# The power of our bootstrapped test is:
set.seed(750)
alt_probs <- ecdf(t_alt)
1 - alt_probs(ci_99[2])
```
The power to reject the null hypothesis equals 60.07%.