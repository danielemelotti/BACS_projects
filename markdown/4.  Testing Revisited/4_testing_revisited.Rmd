---
title: "HW (Week 4) - BACS - Testing Revisited"
author: 'Author: 110077432'
date: "3/11/2022"
output:
  pdf_document: default
header-includes:
- \usepackage{fancyhdr}
- \pagestyle{fancy}
- \fancyhead[CO,CE]{110077432}
- \fancyfoot[CO,CE]{  }
- \fancyfoot[LE,RO]{\thepage}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r,echo=FALSE,message=FALSE,warning=FALSE}
# Set so that long lines in R will be wrapped:
knitr::opts_chunk$set(tidy.opts=list(width.cutoff=80), tidy=TRUE)
```

# Question 1
## a) Calculating the probability that a random app from Google's app store will turn off Verify feature
```{r, include = FALSE}
require(MASS)
require(gridExtra)
```

The probability (represented as a decimal fraction) that a randomly chosen app from Googleâ€™s app store will turn off the Verify security feature is:
```{r}
verify_off <- fractions(pnorm(-3.7), max.denominator = 100000)

f = function(x, den) {paste0(round(x * den), "/", den)}
options("scipen" = 100, "digits"=10)

f(verify_off, 10^13)
```
Else presented as:
```{r}
pnorm(-3.7)
```


## b) Calculating the number of apps on the Play Store that Google expected would maliciously turn off the Verify feature once installed:
```{r}
apps <- 2200000
round(apps * pnorm(-3.7))
```

There are approximately 237 apps on the Play Store which Google would expect to maliciously turn off the Verify feature.

# Question 2

```{r, include = FALSE}
require(data.table)
require(ggplot2)
require(ggthemes)
```

```{r}
# Loading the data:
verizon <- read.csv("../../data/verizon.csv")
```

## a) Null distribution of t-values

### i) Visualizing Verizon's repair times distribution with the mean repair time
```{r fig, tidy=TRUE, tidy.opts = list(blank = FALSE, width.cutoff = 60), fig.height = 3, fig.width = 5, fig.align = "center"}
ggplot(verizon, aes(x = Time)) + 
  geom_density(alpha = 0.3, fill="#FF9999") +
  xlim(-50, 200) +
  xlab("Repair time") +
  ylab("Density") +
  geom_vline(aes(xintercept = mean(Time)),   
             color="red", linetype="dashed", size=1) +
  theme_stata() +
  ggtitle("Verizon's repair times")
```

### ii) Setting the hypotheses
The hypotheses are:
H0: Mean repair time = 7.6 mins vs HA: Mean repair time != 7.6 mins.

### iii) Estimating the population mean and 99% CI
```{r}
verizon <- verizon$Time

# Mean:
mean_time <- mean(verizon)
mean_time

sd_time <- sd(verizon)
se <- sd_time/sqrt(length(verizon))
```

```{r}
# 99% CI:
mean_time + c(-2.58, 2.58) * se
```

### iv) Find t-statistic and p-value of the test
```{r}
# t-statistic:
mu_0 <- 7.6
t_stat <- (mean_time - mu_0)/se
t_stat
```

```{r}
# p-value:
df <- length(verizon) - 1
p_value <- 1 - pt(t_stat, df)
p_value
```

### v) How do the t-statistic and p-value relate to the null distribution of t?

The t-statistic is the ratio of departure of the estimated value of the mean from its hypothesized value to its standard error. If the t-statistic falls out of the 99% CI range (-2.58, 2.58; 3 standard deviations to the left of the mean, and 3 standard deviations to the right) there is a likelihood that the null distribution of t (which is centered on 0) is not really the proper distribution for our data. Then, we'd have to consider the alternative distribution, which shares the same shape with the null distribution. As the number of degrees of freedom is big, we can approximate the t-distribution with a normal distribution. The p-value serves to determine whether or not the t-statistic is statistically significant.

### vi) Conclusion
The test we are conducting is a two-sided t-test. This means that our critical p-value must be halved (it becomes 0.005 for 99% confidence). The t-statistic is smaller (slightly) than 2.58 and the p-value is bigger (slightly) than 0.005, which means that our t-statistic, despite being closer than 3 standard deviations from the mean, is not significant within 99% confidence. Therefore, we cannot reject the null hypothesis within 99% confidence, leading to the conclusion that the claimed repair time of Verizon is likely to be true if we consider a 99% confidence.

## b) Using bootstrapping on sample data

### i) Estimate the 99% bootstrapped CI
```{r}
num_boots <- 2000

compute_sample_mean <- function(sample0) {
  resample <- sample(sample0, length(sample0), replace = TRUE)
  mean(resample)
}

set.seed(150)

boot_99 <- replicate(num_boots, compute_sample_mean(verizon))

perc_99_CI <- quantile(boot_99, probs = c(0.025, 0.995))
perc_99_CI # mu_0 is not included in the interval
```

### ii) Bootstrapped difference of means
```{r}
boot_mean_diffs <- function(sample0, mean_hyp) {
  resample <- sample(sample0, length(sample0), replace=TRUE)
  return( mean(resample) - mean_hyp )
}

set.seed(350)

mean_diffs <- replicate(
  num_boots, 
  boot_mean_diffs(verizon, mu_0)
) 

diff_99_CI <- quantile(mean_diffs, probs=c(0.005, 0.995))
diff_99_CI # 0 is not included!
```

### iii) Bootstrapped t-interval
```{r}
boot_t_stat <- function(sample0, mean_hyp) {
  resample <- sample(sample0, length(sample0), replace=TRUE)
  diff <- mean(resample) - mean_hyp
  se <- sd(resample)/sqrt(length(resample))
  return( diff / se )
}

set.seed(450)

t_boots <- replicate(num_boots, boot_t_stat(verizon, mu_0))

t_99_CI <- quantile(t_boots, probs=c(0.005, 0.995))
t_99_CI # 0 is not included!
```

### iv) Plot separate distributions of the three bootstraps
```{r figs, tidy=TRUE, tidy.opts = list(blank = FALSE, width.cutoff = 60), fig.height = 3, fig.width = 5, fig.align = "center"}
# Bootstrapped percentiles
ggplot(data.frame(boot_99), aes(x = boot_99)) + 
  geom_density(alpha = 0.3, fill="#FF9999") +
  geom_vline(xintercept =  as.numeric(perc_99_CI), color="blue", linetype="dashed") +
  geom_vline(xintercept = as.numeric(mean(boot_99)), linetype = "solid", color = "red", size = 1) +
  theme_stata() +
  xlab("Means") +
  ylab("Density") +
  ggtitle("Bootstrapped Means")

# Bootstrapped difference of means
ggplot(data.frame(mean_diffs), aes(x = mean_diffs)) + 
  geom_density(alpha = 0.3, fill="#FF9999") +
  geom_vline(xintercept =  as.numeric(diff_99_CI), color="blue", linetype="dashed") +
  geom_vline(xintercept = as.numeric(mean(mean_diffs)), linetype = "solid", color = "red", size = 1) +
  theme_stata() +
  xlab("Difference of Means") +
  ylab("Density") +
  ggtitle("Bootstrapped Difference of Means")

# Bootstrapped t-statistics
ggplot(data.frame(t_boots), aes(x = t_boots)) + 
  geom_density(alpha = 0.3, fill="#FF9999") +
  geom_vline(xintercept =  as.numeric(t_99_CI), color="blue", linetype="dashed") +
  geom_vline(xintercept = as.numeric(mean(t_boots)), linetype = "solid", color = "red", size = 1) +
  theme_stata() +
  xlab("t-statistics") +
  ylab("Density") +
  ggtitle("Bootstrapped t-statistics")
```

## c) Do the 4 methods agree with each other on the test?
The traditional test leads to the conclusion that we cannot reject the null hypothesis. According to the other 3 methods, we can reject the null hypothesis and conclude that the mean repair time of Verizon is not equal to 7.6 minutes. However, it is important to underline that these conclusions highly depend on the set.seed() function; setting a different number of seeds always changes the CIs, sometimes denying the rejection of H0.